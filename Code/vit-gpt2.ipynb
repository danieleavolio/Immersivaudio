{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Lovaion\\University\\2nd Year\\Erasmus\\Learning Based Multimedia Processing\\Immersivaudio\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Lovaion\\University\\2nd Year\\Erasmus\\Learning Based Multimedia Processing\\Immersivaudio\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "image_to_text = pipeline(\"image-to-text\", model=\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all imags inside images folder\n",
    "import os\n",
    "images = os.listdir(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Lovaion\\University\\2nd Year\\Erasmus\\Learning Based Multimedia Processing\\Immersivaudio\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'a crowd of people standing around a church '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a basketball game with a player jumping up to the hoop '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a plate of food with meat and vegetables '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a small bus is traveling down the street '}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "\n",
    "for image in images:\n",
    "    result = image_to_text(f\"images/{image}\")\n",
    "    res.append(result)\n",
    "\n",
    "for i in res:\n",
    "    print(i)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with single video frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = os.listdir(\"Scripts/test/testings.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Lovaion\\University\\2nd Year\\Erasmus\\Learning Based Multimedia Processing\\Immersivaudio\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "\n",
    "for frame in frames:\n",
    "    result = image_to_text(f\"Scripts/test/testings.mp4/{frame}\")\n",
    "    res.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'a woman sitting on a couch with a man '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a woman is standing in a room with a group of people '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a man and woman are playing a video game '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a woman is standing next to a man in a room '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a man and a woman are sitting on a bed '}]\n",
      "\n",
      "\n",
      "[{'generated_text': \"a man is holding a woman's hand while she looks at something \"}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a woman is laying on a bed with a man '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a woman holding a remote control in her hand '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a man holding a remote control in his hand '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a man holding a gun and a bottle '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a man holding a remote control in his hand '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a man and a woman sitting on a bed '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a boy holding a plastic bottle and a video game controller '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a man holding a remote control in his hand '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a person is holding a cat on a bed '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a person holding a camera and a baseball glove '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a man is laying on a bed with a cat '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a woman is holding a cup and a bowl '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a woman holding a cup with a man '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a woman holding a bowl of food in her hands '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a woman holding a plate of food with a cup of coffee '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a man holding a bowl of food in his hands '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a man sitting in a chair with a laptop '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a man holding a bowl of food in his hands '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a woman holding a spoon in front of a man '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a young girl is looking at a man '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a man and woman are standing together '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a man is standing in a room with a woman '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a woman sitting next to a man on a bed '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a woman sitting next to a man on a chair '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a man sitting in a chair with a laptop '}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in res:\n",
    "    print(i)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Lovaion\\University\\2nd Year\\Erasmus\\Learning Based Multimedia Processing\\Immersivaudio\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'a cat is looking out of a window '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is sitting on the ground looking at something '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on a sidewalk next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is looking out of a window '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on a sidewalk next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on a sidewalk next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is sitting on the ground looking at something '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on a ledge looking at something '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on a sidewalk next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on a sidewalk next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on a sidewalk next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground next to a tree '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on a ledge looking at something '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is looking at something in the distance '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on a ledge looking at something '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on a ledge looking at the camera '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on a ledge looking at something '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on a tree branch looking at something '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat looking out of a window '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is looking at the camera while standing in the grass '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat looking out of a window '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a dog looking at something in the distance '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat looking out a window at a tree '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is looking out of a window '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is looking at something in the distance '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on a ledge looking at something '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is looking out of a window '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat looking out of a window '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground looking at something '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is looking out of a window '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a dog looking out a window at a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is looking out of a window '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat looking out a window at a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a dog looking at something in the distance '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is laying on the ground with its paw on a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is looking at the camera while sitting on a ledge '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat standing on a sidewalk next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is looking out of a window '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is looking at a cat on the sidewalk '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat is standing on the sidewalk looking at something '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground looking at something '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground looking at something '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground next to a plant '}]\n",
      "\n",
      "\n",
      "[{'generated_text': 'a cat sitting on the ground next to a plant '}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat = os.listdir(\"Scripts/cat/cat.mp4\")\n",
    "\n",
    "res = []\n",
    "\n",
    "for c in cat:\n",
    "    result = image_to_text(f\"Scripts/cat/cat.mp4/{c}\")\n",
    "    res.append(result)\n",
    "\n",
    "for i in res:\n",
    "    print(i)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
