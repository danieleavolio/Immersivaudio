{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from sklearn_extra.cluster import KMedoids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "model = YOLO(\"yolov8n.pt\", verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yolo_labels(path):\n",
    "\n",
    "    images = os.listdir(path)\n",
    "    # sort images by number\n",
    "    images.sort(key=lambda x: int(x.split(\".\")[0]))\n",
    "\n",
    "    res = model(path, conf=0.25)\n",
    "\n",
    "    result = []\n",
    "    for ix, r in enumerate(res):\n",
    "        els = set()\n",
    "        # If there are no boxes, skip\n",
    "        try:\n",
    "            # use r.boxes and not r[0].boxes to get all boxes\n",
    "            for c in r.boxes.cls:\n",
    "                els.add(model.names[int(c)])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        result.append({\"frame\": ix + 1, \"labels\": list(els)})\n",
    "\n",
    "    json_result = json.dumps(result)\n",
    "\n",
    "    return json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/14 D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\\1.jpg: 384x640 1 cat, 18.5ms\n",
      "image 2/14 D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\\10.jpg: 384x640 1 cat, 18.3ms\n",
      "image 3/14 D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\\11.jpg: 384x640 1 cat, 19.3ms\n",
      "image 4/14 D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\\12.jpg: 384x640 1 cat, 1 dog, 18.5ms\n",
      "image 5/14 D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\\13.jpg: 384x640 1 cat, 18.0ms\n",
      "image 6/14 D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\\14.jpg: 384x640 1 cat, 18.4ms\n",
      "image 7/14 D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\\2.jpg: 384x640 1 cat, 18.9ms\n",
      "image 8/14 D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\\3.jpg: 384x640 1 cat, 16.2ms\n",
      "image 9/14 D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\\4.jpg: 384x640 1 cat, 9.0ms\n",
      "image 10/14 D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\\5.jpg: 384x640 1 cat, 8.5ms\n",
      "image 11/14 D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\\6.jpg: 384x640 1 cat, 9.4ms\n",
      "image 12/14 D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\\7.jpg: 384x640 1 cat, 8.0ms\n",
      "image 13/14 D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\\8.jpg: 384x640 1 cat, 17.0ms\n",
      "image 14/14 D:\\programming\\Multilearning\\immersiveaudio\\Code\\Scripts\\output\\cat\\9.jpg: 384x640 1 cat, 18.1ms\n",
      "Speed: 1.6ms preprocess, 15.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[{\"frame\": 1, \"labels\": [\"cat\"]}, {\"frame\": 2, \"labels\": [\"cat\"]}, {\"frame\": 3, \"labels\": [\"cat\"]}, {\"frame\": 4, \"labels\": [\"dog\", \"cat\"]}, {\"frame\": 5, \"labels\": [\"cat\"]}, {\"frame\": 6, \"labels\": [\"cat\"]}, {\"frame\": 7, \"labels\": [\"cat\"]}, {\"frame\": 8, \"labels\": [\"cat\"]}, {\"frame\": 9, \"labels\": [\"cat\"]}, {\"frame\": 10, \"labels\": [\"cat\"]}, {\"frame\": 11, \"labels\": [\"cat\"]}, {\"frame\": 12, \"labels\": [\"cat\"]}, {\"frame\": 13, \"labels\": [\"cat\"]}, {\"frame\": 14, \"labels\": [\"cat\"]}]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_yolo_labels(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Running on gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\tedoc/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "d:\\programming\\Multilearning\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\programming\\Multilearning\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! Running on gpu\")\n",
    "    model = model.to('cuda')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_frame(path):\n",
    "\n",
    "    inputPath = path\n",
    "    frames_number = 14\n",
    "    video_id = 1\n",
    "\n",
    "\n",
    "    # Load the images\n",
    "    images = {}\n",
    "    for i in range(1,frames_number + 1):\n",
    "        img_name = inputPath + \"/\" + str(i) + \".jpg\" \n",
    "        img = Image.open(img_name)\n",
    "        images[img_name] = (img)\n",
    "\n",
    "\n",
    "    activation = None\n",
    "\n",
    "    def hook(model, input, output):\n",
    "        nonlocal activation\n",
    "        activation = output\n",
    "\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(512),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    latent = {}\n",
    "    for i in images:\n",
    "        input_tensor = preprocess(images[i])   \n",
    "        input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "        # move the input and model to GPU for speed if available\n",
    "        if torch.cuda.is_available():\n",
    "            input_batch = input_batch.to('cuda')\n",
    "\n",
    "        model.layer3[1].conv2.register_forward_hook(hook)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model(input_batch)\n",
    "        latent[i] = activation[0]\n",
    "\n",
    "    #  initial shape 512 x 16 x 29, need to reshape everything to 1 x (512*16*29) then stack to \n",
    "    flattened_arrays = [arr.cpu().flatten() for arr in latent.values()]\n",
    "\n",
    "    flattened_data = np.vstack(flattened_arrays)\n",
    "    \n",
    "    med_model = KMedoids(n_clusters=1, random_state=0)\n",
    "   \n",
    "    med_model.fit(flattened_data)\n",
    "\n",
    "    # Trovare il medoide\n",
    "    medoid_index = med_model.medoid_indices_[0]\n",
    "\n",
    "    return json.dumps({\n",
    "        \"video_id\" : video_id,\n",
    "        \"best_frame\" : list(images.keys())[medoid_index],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"video_id\": 1, \"best_frame\": \"D:\\\\\\\\programming\\\\\\\\Multilearning\\\\\\\\immersiveaudio\\\\\\\\Code\\\\\\\\Scripts\\\\\\\\output\\\\\\\\cat/10.jpg\"}'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_frame(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
